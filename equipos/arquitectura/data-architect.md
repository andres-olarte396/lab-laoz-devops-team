# Data Architect

## üìã Visi√≥n General

El Data Architect es responsable de dise√±ar, implementar, y gobernar la **estrategia de datos** de la organizaci√≥n, asegurando que los datos sean accesibles, confiables, seguros, y que generen valor de negocio. Act√∫a como puente entre necesidades de negocio, analytics, y sistemas transaccionales.

## üéØ Responsabilidades

### Dise√±o de Arquitectura de Datos

**Principales tareas**:
- Dise√±ar data models (conceptual, logical, physical)
- Definir estrategia de data warehouse / data lake
- Arquitectura de data pipelines (ETL/ELT)
- Real-time streaming architecture
- Master Data Management (MDM) strategy

**Entregables**:
- Entity-Relationship Diagrams (ERD)
- Data flow diagrams
- Data architecture blueprints
- Schema design documentation
- Integration patterns

---

### Data Governance

**Principales tareas**:
- Definir pol√≠ticas de data governance
- Data quality standards y monitoring
- Data lineage tracking
- PII (Personally Identifiable Information) protection
- Compliance (GDPR, CCPA, HIPAA seg√∫n industria)
- Data retention policies

**Framework de governance**:
```yaml
Data Ownership:
  - Data Stewards: Business owners de cada dominio
  - Data Custodians: Technical owners (Data Engineers)
  - Data Governance Committee: Cross-functional oversight

Data Quality Dimensions:
  - Accuracy: ¬øDatos correctos?
  - Completeness: ¬øSin nulls cr√≠ticos?
  - Consistency: ¬øCoherente cross-systems?
  - Timeliness: ¬øActualizado cuando necesario?
  - Uniqueness: ¬øSin duplicados?
  - Validity: ¬øDentro de rangos esperados?

Privacy & Security:
  - Data classification (Public, Internal, Confidential, Restricted)
  - Encryption at rest & in transit
  - Access controls (RBAC, ABAC)
  - Anonymization / Pseudonymization
  - Audit logging
```

---

### Data Platform Architecture

**Principales tareas**:
- Selecci√≥n de tecnolog√≠as de datos (SQL, NoSQL, streaming)
- Data warehouse design (dimensional modeling, star/snowflake schema)
- Data lake architecture (zones: raw, curated, enriched)
- Lakehouse strategy (Delta Lake, Iceberg)
- Analytics platform (BI, ML, self-service)

**Arquitectura t√≠pica**:
```
Data Sources (Transactional DBs, APIs, Files, Streaming)
    ‚Üì
Ingestion Layer (Azure Data Factory, Fivetran, Airbyte, Kafka)
    ‚Üì
Raw Data Zone (Data Lake: Parquet, Avro)
    ‚Üì
Transform Layer (Databricks, Spark, dbt)
    ‚Üì
Curated Data Zone (Data Warehouse: Dimensional models)
    ‚Üì
Serving Layer (BI: Power BI, Tableau; ML: Feature Store; API: GraphQL)
    ‚Üì
Consumption (Dashboards, Reports, ML Models, Applications)
```

---

### Data Integration

**Principales tareas**:
- Dise√±ar estrategia de integraci√≥n de datos
- API design para data access
- Event-driven data architecture
- Change Data Capture (CDC) patterns
- Data federation vs consolidation

**Patrones de integraci√≥n**:
- **Batch ETL**: Scheduled jobs (diario, semanal)
- **Real-time streaming**: Kafka, Event Hubs, Kinesis
- **Micro-batch**: Spark Structured Streaming
- **CDC (Change Data Capture)**: Debezium, Azure SQL CDC
- **Reverse ETL**: Warehouse ‚Üí Operational systems

---

### Analytics & ML Platform

**Principales tareas**:
- Feature store design para ML
- Data catalog implementation
- Self-service analytics enablement
- Semantic layer design
- Metrics store architecture

**Componentes**:
```yaml
Data Catalog:
  - Tool: Azure Purview, Alation, Collibra
  - Purpose: Data discovery, metadata management, lineage

Feature Store:
  - Tool: Feast, Tecton, Azure ML Feature Store
  - Purpose: ML feature reusability, consistency

Semantic Layer:
  - Tool: dbt Metrics, Cube.js, Metabase
  - Purpose: Business logic centralization

Metrics Store:
  - Tool: Transform (dbt metrics), GoodData
  - Purpose: Definici√≥n √∫nica de m√©tricas de negocio
```

---

### Performance & Optimization

**Principales tareas**:
- Database performance tuning
- Query optimization
- Indexing strategies
- Partitioning & sharding
- Caching layers (Redis, Memcached)
- Cost optimization (storage tiering, compression)

**M√©tricas de performance**:
- Query latency: p50, p95, p99
- Data freshness: SLAs por dataset
- Pipeline execution time
- Storage costs per TB
- Compute costs (queries, transformations)

---

## üíº Perfil del Rol

### Seniority

**Nivel**: Senior a Staff (8-15 a√±os de experiencia)

**Progresi√≥n t√≠pica**:
```
Data Engineer (3-5 a√±os)
    ‚Üì
Senior Data Engineer (5-8 a√±os)
    ‚Üì
Data Architect (8-12 a√±os)
    ‚Üì
Senior Data Architect (12-15 a√±os)
    ‚Üì
Principal Data Architect / Chief Data Officer (15+ a√±os)
```

---

### Skills Requeridas

#### Technical Skills (Deep)

**Must have**:
- ‚úÖ **SQL**: Advanced (window functions, CTEs, query optimization)
- ‚úÖ **Database design**: Normalization, dimensional modeling, indexing
- ‚úÖ **Data warehousing**: Star/snowflake schema, SCD (Slowly Changing Dimensions)
- ‚úÖ **ETL/ELT**: Azure Data Factory, Airflow, dbt, Spark
- ‚úÖ **Big Data**: Spark, Databricks, distributed computing concepts
- ‚úÖ **Cloud data platforms**: Azure (Synapse, Data Lake, Fabric) o AWS (Redshift, S3, Glue)
- ‚úÖ **Streaming**: Kafka, Event Hubs, Flink, Spark Streaming
- ‚úÖ **NoSQL**: MongoDB, Cosmos DB, Cassandra, Redis

**Nice to have**:
- üî∂ **Data science fundamentals**: Para colaborar con Data Scientists
- üî∂ **ML Ops**: Model deployment, feature engineering
- üî∂ **Graph databases**: Neo4j, CosmosDB Gremlin
- üî∂ **Time-series databases**: InfluxDB, TimescaleDB

---

#### Data Modeling (Critical)

**Must have**:
- ‚úÖ **Conceptual modeling**: Entity-Relationship diagrams
- ‚úÖ **Logical modeling**: Normalized models (3NF, BCNF)
- ‚úÖ **Physical modeling**: Indexes, partitions, storage optimization
- ‚úÖ **Dimensional modeling**: Kimball methodology, star schema
- ‚úÖ **Data Vault**: Para enterprise data warehouses
- ‚úÖ **Graph modeling**: Si hay use cases de grafo

---

#### Data Governance & Security

**Must have**:
- ‚úÖ **Data governance frameworks**: DAMA-DMBOK, DGI Framework
- ‚úÖ **Data quality**: Great Expectations, dbt tests, Soda
- ‚úÖ **Privacy regulations**: GDPR, CCPA compliance
- ‚úÖ **Security**: Encryption, masking, tokenization, RBAC
- ‚úÖ **Data lineage**: Tools como Azure Purview, Alation

---

#### Business Skills (Medium)

**Must have**:
- ‚úÖ **Domain knowledge**: Entender el negocio profundamente
- ‚úÖ **Analytics thinking**: Traducir preguntas de negocio a datos
- ‚úÖ **Stakeholder management**: Comunicar con business users
- ‚úÖ **ROI analysis**: Justificar inversiones en data infrastructure

---

### Stack Tecnol√≥gico

El Data Architect debe dominar el stack de datos moderno:

#### Databases

**Relational (OLTP)**:
```yaml
Azure SQL Database / SQL Server
PostgreSQL
MySQL
Oracle (legacy enterprises)
```

**Data Warehouses (OLAP)**:
```yaml
Azure Synapse Analytics
Snowflake
Amazon Redshift
Google BigQuery
Databricks SQL
```

**NoSQL**:
```yaml
Document: MongoDB, Cosmos DB
Key-Value: Redis, DynamoDB
Column-family: Cassandra, HBase
Graph: Neo4j, Cosmos DB Gremlin
Time-series: InfluxDB, TimescaleDB
```

---

#### Data Processing

**Batch Processing**:
```yaml
Azure Data Factory (orchestration)
Apache Spark (Databricks, Synapse Spark)
dbt (analytics engineering)
Python (pandas, polars)
```

**Streaming**:
```yaml
Apache Kafka
Azure Event Hubs
Apache Flink
Spark Structured Streaming
Kafka Streams
```

---

#### Data Storage

**Data Lake**:
```yaml
Azure Data Lake Storage Gen2 (ADLS)
AWS S3
Google Cloud Storage
File formats: Parquet, Avro, Delta Lake, Iceberg
```

**Data Warehouse**:
```yaml
Azure Synapse dedicated SQL pools
Snowflake
Redshift
```

---

#### Data Governance & Quality

```yaml
Catalog: Azure Purview, Alation, Collibra, DataHub
Quality: Great Expectations, Soda, dbt tests
Lineage: Azure Purview, OpenLineage
Master Data: Informatica MDM, Profisee
```

---

#### Analytics & BI

```yaml
BI Tools: Power BI, Tableau, Looker, Metabase
Semantic Layer: dbt Metrics, Cube.js
ML Platforms: Azure ML, Databricks MLflow
Notebooks: Jupyter, Databricks, Synapse Notebooks
```

---

## üìä M√©tricas de √âxito

### Data Platform KPIs

| M√©trica | Target | Frecuencia |
|---------|--------|------------|
| **Data Availability** | >99.9% | Mensual |
| **Data Freshness SLA** | >95% datasets on time | Diaria |
| **Pipeline Success Rate** | >98% | Diaria |
| **Query Performance** | p95 <5s (BI queries) | Semanal |
| **Storage Cost Efficiency** | <$X per TB/month | Mensual |

### Data Quality KPIs

| M√©trica | Target | Frecuencia |
|---------|--------|------------|
| **Data Quality Score** | >95% | Semanal |
| **Critical Data Issues** | 0 | Diaria |
| **Completeness** | >98% (non-null en campos cr√≠ticos) | Semanal |
| **Accuracy** | >99% (validated against source) | Mensual |
| **Duplicates** | <0.1% | Semanal |

### Governance KPIs

| M√©trica | Target | Frecuencia |
|---------|--------|------------|
| **Data Catalog Coverage** | >90% datasets catalogados | Mensual |
| **PII Compliance** | 100% PII protegido | Continuo |
| **Access Control Audit** | 0 unauthorized access | Trimestral |
| **Data Lineage Coverage** | >80% pipelines documented | Trimestral |
| **Retention Policy Compliance** | 100% | Trimestral |

### Business Impact KPIs

| M√©trica | Target | Frecuencia |
|---------|--------|------------|
| **Self-Service Adoption** | >60% analysts usando platform | Trimestral |
| **Time to Insight** | -25% YoY | Trimestral |
| **Data-Driven Decisions** | >80% product decisions con datos | Trimestral |
| **ML Model Performance** | Dependent on use case | Mensual |

---

## üîÑ Interacciones con Otros Equipos

### Con Data Engineering Team

**Frecuencia**: Daily a Weekly  
**Modo**: **Facilitating** + **Collaboration**

**Actividades**:
- Dise√±o de data pipelines
- Code reviews de transformaciones complejas
- Performance optimization
- Best practices sharing
- Architecture decisions

**Tools**: Daily standups, Slack (#data-engineering), GitHub, Databricks

---

### Con Data Science / ML Team

**Frecuencia**: Weekly  
**Modo**: **Collaboration**

**Actividades**:
- Feature store design
- Data requirements para modelos
- Model serving infrastructure
- ML Ops pipelines
- Exploratory data analysis support

**Tools**: Weekly sync, Jupyter notebooks, ML platform (Azure ML, Databricks)

---

### Con Analytics / BI Team

**Frecuencia**: Weekly  
**Modo**: **X-as-a-Service** + **Facilitating**

**Actividades**:
- Semantic layer design
- BI data mart creation
- Query optimization
- Self-service enablement
- Metrics definitions

**Tools**: Power BI, dbt, Slack (#analytics)

---

### Con Application Developers

**Frecuencia**: Weekly  
**Modo**: **Collaboration**

**Actividades**:
- Database schema design
- API design para data access
- Performance troubleshooting
- CDC implementation
- Caching strategies

**Tools**: GitHub, Slack (#development), Architecture reviews

---

### Con Product Team

**Frecuencia**: Bi-weekly  
**Modo**: **Facilitating**

**Actividades**:
- Data requirements gathering
- Feasibility assessments
- Analytics roadmap alignment
- Business metric definitions
- User behavior data design

**Tools**: Product roadmap reviews, Confluence

---

### Con Security / Compliance

**Frecuencia**: Monthly  
**Modo**: **Collaboration**

**Actividades**:
- Data security reviews
- PII protection strategies
- Compliance audits (GDPR, etc.)
- Access control policies
- Encryption standards

**Tools**: Security reviews, Compliance dashboards

---

### Con Enterprise Architect

**Frecuencia**: Monthly  
**Modo**: **Facilitating** (recibiendo guidance)

**Actividades**:
- Data strategy alignment
- Cross-system data integration
- Technology evaluations
- Enterprise data governance
- ADR reviews

**Tools**: Architecture forum, ADRs, Monthly syncs

---

## üéì Desarrollo Profesional

### Path de Carrera

#### Opci√≥n 1: Technical Depth (IC Track)

```
Data Architect (8-12 a√±os)
    ‚Üì
Senior Data Architect (12-15 a√±os)
    - Scope: Enterprise data platform
    - Impact: Org-wide data strategy
    ‚Üì
Principal Data Architect (15-20 a√±os)
    - Scope: Multi-product data ecosystems
    - Impact: Industry thought leadership
    ‚Üì
Distinguished Engineer - Data (20+ a√±os)
    - Scope: Technology vision
    - Impact: External influence
```

#### Opci√≥n 2: Leadership (Management Track)

```
Data Architect (8-12 a√±os)
    ‚Üì
Head of Data Architecture (10-14 a√±os)
    - Manage: 2-5 data architects/engineers
    - Scope: Data platform team
    ‚Üì
Director of Data Engineering (14-18 a√±os)
    - Manage: 10-30 data professionals
    - Scope: Entire data organization
    ‚Üì
Chief Data Officer (CDO) (18+ a√±os)
    - Manage: Data + Analytics + ML org
    - Scope: Enterprise data strategy
```

---

### Skills a Desarrollar

**Pr√≥ximos 6-12 meses**:
- [ ] Certificaci√≥n avanzada cloud data (Azure Data Engineer Associate + Expert, AWS Data Analytics)
- [ ] Implementar data quality framework (Great Expectations / Soda)
- [ ] Desplegar data catalog (Azure Purview / Alation)
- [ ] Mentoring de 1-2 Data Engineers
- [ ] Presentar 2-3 tech talks sobre data architecture

**Pr√≥ximos 1-2 a√±os**:
- [ ] Liderar migraci√≥n data warehouse a cloud
- [ ] Implementar real-time streaming platform
- [ ] Build feature store para ML
- [ ] Desarrollar data governance program
- [ ] Contribuir a open source data tools

**Pr√≥ximos 3-5 a√±os** (hacia Principal / CDO):
- [ ] Data strategy enterprise-wide
- [ ] Thought leadership (conferencias, blogs)
- [ ] Cross-industry data expertise
- [ ] Business impact demostrable
- [ ] Team building (contratar data team)

---

### Recursos de Aprendizaje

#### Libros Esenciales

- üìö **"Designing Data-Intensive Applications"** - Martin Kleppmann (must-read)
- üìö **"The Data Warehouse Toolkit"** - Ralph Kimball (dimensional modeling bible)
- üìö **"Fundamentals of Data Engineering"** - Joe Reis & Matt Housley (2022)
- üìö **"Data Mesh"** - Zhamak Dehghani (modern data architecture)
- üìö **"Building a Data Lakehouse"** - Bill Inmon et al.
- üìö **"The Enterprise Big Data Lake"** - Alex Gorelik

#### Cursos y Certificaciones

**Azure**:
- Microsoft Certified: Azure Data Engineer Associate (DP-203)
- Microsoft Certified: Azure Data Scientist Associate (DP-100)
- Microsoft Certified: Azure Database Administrator Associate (DP-300)
- Databricks Data Engineer Professional

**AWS**:
- AWS Certified Data Analytics - Specialty
- AWS Certified Database - Specialty

**Otros**:
- dbt Analytics Engineering Certification
- Snowflake SnowPro Core/Advanced Certifications
- CDMP (Certified Data Management Professional) - DAMA

#### Comunidades

- **Data Engineering Weekly**: Newsletter
- **dbt Community Slack**: Analytics engineering
- **Locally Optimistic**: Data newsletter/slack
- **DataTalks.Club**: Comunidad de data engineering
- **Databricks Community**: Cloud data platform

---

## üìù Herramientas del D√≠a a D√≠a

### Data Modeling

| Tool | Uso | Nivel |
|------|-----|-------|
| **ERwin** | Enterprise data modeling | Advanced |
| **Lucidchart** | ERD diagrams | Intermediate |
| **dbdiagram.io** | Quick database schema design | Basic |
| **dbt** | Analytics data modeling | Advanced |

### Development & Transformation

| Tool | Uso |
|------|-----|
| **SQL (T-SQL, PostgreSQL)** | Query development, optimization |
| **Python** | Data pipelines, automation |
| **dbt** | Analytics transformations, testing |
| **Databricks / Synapse** | Big data processing |
| **Spark (PySpark)** | Large-scale data processing |

### Governance & Quality

| Tool | Uso |
|------|-----|
| **Azure Purview** | Data catalog, lineage |
| **Great Expectations** | Data quality validation |
| **Soda** | Data quality monitoring |
| **Collibra / Alation** | Data governance platform |

### Orchestration & Monitoring

| Tool | Uso |
|------|-----|
| **Azure Data Factory** | ETL orchestration |
| **Apache Airflow** | Workflow orchestration |
| **dbt Cloud** | Analytics workflow management |
| **Grafana / Azure Monitor** | Pipeline monitoring |

---

## üöÄ Ejemplo de Semana T√≠pica

### Lunes
- **9:00-9:30**: Review de data quality alerts del fin de semana
- **10:00-11:00**: Sync con Data Engineering team (sprint planning)
- **11:00-12:00**: Design session: Nueva data pipeline con Data Engineer
- **14:00-15:00**: 1:1 con Data Scientist (feature store requirements)
- **15:00-17:00**: Deep work: Dimensional model design para BI

### Martes
- **9:00-10:00**: Data governance meeting (PII compliance review)
- **10:00-12:00**: Deep work: Performance tuning de queries lentas
- **14:00-15:00**: Analytics roadmap sync con Product Manager
- **15:00-17:00**: Code review de dbt models + Slack support

### Mi√©rcoles
- **9:00-10:00**: 1:1 con Enterprise Architect (data strategy alignment)
- **10:00-12:00**: POC: Evaluar nueva tecnolog√≠a (e.g., Iceberg table format)
- **14:00-15:30**: Architecture review: Real-time streaming design
- **15:30-17:00**: Deep work: Data catalog metadata enrichment

### Jueves
- **9:00-10:00**: Security review: Data access controls audit
- **10:00-12:00**: Pair programming con Data Engineer (complex CDC pipeline)
- **14:00-15:00**: BI team sync: Semantic layer design
- **15:00-17:00**: Deep work: Data quality framework implementation

### Viernes
- **9:00-10:00**: Review de m√©tricas (pipeline SLAs, data quality scores)
- **10:00-11:00**: Tech talk: Presentar dimensional modeling best practices
- **11:00-12:00**: 1:1 mentoring con junior Data Engineer
- **14:00-16:00**: Deep work: Documentation (ADRs, data dictionary)
- **16:00-17:00**: Week wrap-up, backlog grooming

**Deep work**: ~35% del tiempo  
**Meetings & Collaboration**: ~35% del tiempo  
**Code/Design Reviews**: ~20% del tiempo  
**Documentation & Governance**: ~10% del tiempo

---

## üéØ Se√±ales de que est√°s listo para este rol

‚úÖ **Tienes**:
- 8+ a√±os de experiencia con datos (Data Engineer, Database Developer)
- Profundo conocimiento de SQL y data modeling
- Experiencia dise√±ando data warehouses en producci√≥n
- Track record de optimizaci√≥n de performance
- Conocimiento de al menos 2 cloud data platforms

‚úÖ **Puedes**:
- Dise√±ar un data warehouse dimensional desde cero
- Optimizar queries complejas a escala
- Evaluar trade-offs de tecnolog√≠as de datos
- Implementar data governance practices
- Comunicar arquitectura de datos a stakeholders no-t√©cnicos

‚úÖ **Te gusta**:
- Resolver problemas de data modeling complejos
- Trabajar con grandes vol√∫menes de datos
- Garantizar calidad y governance de datos
- Habilitar analytics y ML con datos
- Aprender nuevas tecnolog√≠as de datos constantemente

---

## üîó Links Relacionados

- [Solution Architect](solution-architect.md) - Arquitectura de soluciones
- [Enterprise Architect](enterprise-architect.md) - Arquitectura empresarial
- [Equipo de Arquitectura](README.md) - Visi√≥n general del equipo

---

**√öltima actualizaci√≥n**: Diciembre 2025  
**Mantenido por**: Data Architect / Enterprise Architect